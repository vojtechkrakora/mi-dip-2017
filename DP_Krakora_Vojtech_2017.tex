% options:
% thesis=B bachelor's thesis
% thesis=M master's thesis
% czech thesis in Czech language
% english thesis in English language
% hidelinks remove colour boxes around hyperlinks

\documentclass[thesis=M,czech]{FITthesis}[2012/10/20]

 \usepackage[utf8]{inputenc} % LaTeX source encoded as UTF-8
% \usepackage[latin2]{inputenc} % LaTeX source encoded as ISO-8859-2
% \usepackage[cp1250]{inputenc} % LaTeX source encoded as Windows-1250

\usepackage{graphicx} %graphics files inclusion
% \usepackage{subfig} %subfigures
% \usepackage{amsmath} %advanced maths
% \usepackage{amssymb} %additional math symbols

\usepackage{dirtree} %directory tree visualisation
\usepackage{svg}
\usepackage{amsmath}

% % list of acronyms
% \usepackage[acronym,nonumberlist,toc,numberedsection=autolabel]{glossaries}
% \iflanguage{czech}{\renewcommand*{\acronymname}{Seznam pou{\v z}it{\' y}ch zkratek}}{}
% \makeglossaries


\setlength{\fboxsep}{0.005pt}
\newcommand{\tmpframe}[1]{\fbox{#1}}
%\renewcommand{\tmpframe}[1]{#1}

% % % % % % % % % % % % % % % % % % % % % % % % % % % % % % 
% EDIT THIS
% % % % % % % % % % % % % % % % % % % % % % % % % % % % % % 

\department{Department of Katedra teoretické informatiky}
\title{Analýza bezpečnostních rizik aplikací z logů v reálném čase}
\authorGN{Vojtěch} %author's given name/names
\authorFN{Krákora} %author's surname
\author{Vojtěch Krákora} %author's name without academic degrees
\authorWithDegrees{Bc. Vojtěch Krákora} %author's name with academic degrees
\supervisor{Pavel Pivoňka, GWCPM}
\acknowledgements{THANKS (remove entirely in case you do not with to thank anyone)}
\abstractEN{Summarize the contents and contribution of your work in a few sentences in English language.}
\abstractCS{V n{\v e}kolika v{\v e}t{\' a}ch shr{\v n}te obsah a p{\v r}{\' i}nos t{\' e}to pr{\' a}ce v {\v c}esk{\' e}m jazyce.}
\placeForDeclarationOfAuthenticity{Prague}
\keywordsCS{Replace with comma-separated list of keywords in Czech.}
\keywordsEN{Replace with comma-separated list of keywords in English.}
\declarationOfAuthenticityOption{1} %select as appropriate, according to the desired license (integer 1-6)
% \website{http://site.example/thesis} %optional thesis URL


\usepackage{xcolor} 
\newcommand{\todo}[1]{\textcolor{red}{\textbf{[[#1]]}}}

\usepackage{blindtext}
\newcommand{\blind}[1][1]{\textcolor{gray}{\Blindtext[#1][1]}}



\begin{document}

% \newacronym{CVUT}{{\v C}VUT}{{\v C}esk{\' e} vysok{\' e} u{\v c}en{\' i} technick{\' e} v Praze}
% \newacronym{FIT}{FIT}{Fakulta informa{\v c}n{\' i}ch technologi{\' i}}

\setsecnumdepth{part}
\chapter{Introduction}
\todo{Napsat max jednu stranku}
S roustoucím významem informačních systémů a informací jenž jsou zpracovány počítaočovými systémy je kladen důraz na zabezpečení informací. Bezpečnost informačních systémů je zajišťována zákonem o kybernetické bezpečnosti 181/2014 Sb \cite{zakon181-2014}. Mnoho systémů vyžaduje i vyšší zabezpečení například pomocí ISO normy 27001 \cite{iso27001}.

\blind[2]


\setsecnumdepth{all}
\chapter{Úvod do problematiky}
	
	
	\section{Kybernetická bezpečnost}
		Pro dnešní dobu je běžné využívání informačních technologíích prakticky kdekoliv. Jde o stavební kámen mnoha podniků. Informačním systémem rozumíme kombinaci softwaru, hardwaru, infrastruktury a trénovaného personálu\cite{businessdictionary}.  Tam, kde  se informační systémy vyskytují pomáhájí práci zjednodušovat, tvořit či se jinak podílet. Systémy je třeba udržovat v provozu a co nejvíce se vyhnout všem možným rizikům, které mohou businessový proces narušit.
		
		Obecně lze na zabezpečení informačního systému nahlížet z mnoha různých úhlů. Zabezpečit lze síť pomocí certifikovaného přístupu, správně zabezpečené bezdrátové sítě a podobně. Do operačních systémů se nainstalují antivirové programy. Samotná zařízení se fyzicky uzamknou a znemožní se přístup nepovolaným osobám.
		
		Povinnost zabezpečovat informační systémy přikládá zákon o kybernetické bezpečnosti 181/2014 Sb. Tento zákon stanovuje povinnosti a práva orgánů veřejné moci v oblasti kybernetické bezpečnosti \cite{zakon181-2014}. Tento zákon mimo jiné nařizuje orgánům veřejné moci \todo{Definovat orgán veřejné moci} povinnost zajišťovat kybernetickou bezpečnost.
		
		Kromě zákonu jsou k dispozici normy. Jednou z takových norem je norma ISO 27001. Tato norma stanovuje požadavky, které je nutné dodržovat pro kybernetickou bezpečnost. Tyto požadavky jsou stanoveny jak na samotné informační systémy, tak i na zaměstnance, informační procesy, či strategie firmy. \cite{iso27001}.
		
		Lze říci, že zajištění kybernetické bezpečnosti a následné detekci jednotlivých bezpečnostních rizik je velmi specifické. Každá danná doména má své rizika a jiné druhy způsobů jejich detekce. 
		
		V práci řeším kybernetickou bezpečnost pro konkrétní část informačního systémy. Podstatná tedy jsou bezpečností rizika na integrační platformě Unify. Definice integrační platformy a její popis je v sekci \ref{sec:unify}.
		
		\subsection{Bezpečnostní rizika}
		\todo{Nadefinovat bezpečnostní rizika}
			Jak již bylo uvedeno pro tuto práci jsou podstatná bezpečností rizika na konkrétní integrační platformě. Cílem je hledat pouze ta, která vznikají na straně programu. Zabezpečení přístupů fyzicky k serverům Unify a podobně nebudou probírána.
			
			\subsubsection{Zero day útoky}
				Zero day útoky lze volně přeložit jako útoky nultého dne.
				Jde o taková bezpečnostní rizika, která využívají zranitelností nějakého systému, nebo jeho části, která nebyla zveřejněna \cite{investigationZ0}.
				
				Integrační platforma je složena z mnoha modulů, který tvoří její celek. S rostoucím množstvím softwaru a použitých knihoven jsou šance na skrytou chybu velmi vysoké. 
			
			\subsubsection{Odepření služby}
				Útoky, jejichž cílem je znepřístupnit konkretní jednu či sadu služeb rozhodně patří mezi narušení kybernetické bezpečnosti. Útok spadající do kategorie Dos \todo{Definovat Dos} je DDos, jde o útok, kde hromadně více zařízení zahltí požadavky konkrétní služby a vyřadí je tím z provozu \cite{dosAndDdos}.
				
				\begin{figure}[htb]\centering
					\tmpframe{\includegraphics[width=\textwidth]{./img/todo}}		
					\caption{Grafické znázornění DDos útoku.}
					\label{fig:ddos}
				\end{figure}
				
				Problémem nemusí být když se takové riziko projevuje přímo. Pokud například důležitý partner je pod takovým útokem, ohrožuje to business procesy i na naší straně. Je třeba zavčas podobný incident detekovat a na základě dalších informací se rozhodnout jak vzniklou situaci řešit.
				
				\todo{Chtěl bych zde konkrétně popsat DDos na O2 z lonskeho roku}
			
			\subsubsection{Nevalidní dotazy}
				Riziko, které je třeba hlídat, ale nutně způsobené za účelem poškodit někoho nebo něco. Integrační platforma musí zpracovávat různé požadavky od konzumentů a ty zpravidla přeposílat poskytovatelům služeb. Mohou nastat situace, kdy konzument posílá nevalidní požadavek.
			
				V případě, že požadavky jsou v xml formě, lze definovat přesné znění zprávy. V případě, že je její validita narušena dochází k nekompatibilnímu dotazu na stranu poskytovatele. To může vyústit v nevyřízené a odmítnuté požadavky. Z této situace můhou vzniknout nechtěné útoky \textit{odepření služeb}. V případě, že požadavek nebude zamítnut, možným důsledkem může být zisk nevalidních dat nebo dokonce i dat, která se ke konzumentovi neměla dostat.
			
				Tyto popřípadě i jiné neočekávané situace je třeba včas odhalit a řešit.
				
			\subsection{Ostatní}
				Mnoho bezpečnostních rizik může být neznámých. Nelze vše zaškatulkovat do obecných kategorií. Rizikem mohou být i služby, které po autorizaci nabízí různé možnosti získání dat, například pomocí selectů z databáze.   	
			
		\subsection{Zjišťování bezpečnostních rizik}
			Rozpoznávat bezpečnostní rizika a je při jejich různorodosti složitý úkol. 
			
			\subsubsection{Posouzení kódu}
				Posouzení kódu ( anglicky code review) je metoda kontroly kvality kódu. Programátor, který napsal nějaká kus komponenty dá svou část programu na kontrolu druhé osobě. Není přímo nutné, aby kontrolující osoba byla nadřízený. Kontrolující programátor hlídá kvalitu kódu a zároveň prověřuje jeho funkčnost. Tato metoda vede ke zkvalitnění dodávaného softwaru, ale je finančně i časově náročná. \todo{cite}
			
			Zvýšením kontroly se i zvyšuje šance na detekci bezpečnostního rizika. Jako velmi efektivní metodu ji uvádí zdroj \cite{pethukovKozlovVulnerabilities}.
			
			\subsubsection{Analýza z pohledu uživatele}
				Ve snaze celý proces detekce bezpečnostních rizik zautomatizovat se používá i metoda založená na pohledu uživatele aplikace \cite{pethukovKozlovVulnerabilities}. Aplikace má své uživatele, proto je možné vytvářet scénáře, kdy se uživatel pokouší najít v aplikaci nějaký nedostatek. Takový nedostatek může následně produkovat bezpečnostní riziko. Automatizace těchto scénářů je zpravidla snadno proveditelná a často patří do klasických testovacích scénářů.
				
				Testují se například SQL injection, přístup do neopravněných míst a další.
				
			\subsection{Analýza na straně serveru}
				Předchozí způsoby se snažili zabránit výskytu rizik. Pro zachování kybernetické bezpečnosti je vhodné sledovat i aktuální situaci v aplikaci. Vzhledem k rozshahu moderních aplikací je takřka vyloučené, že bychom dokázali rizikům předejít. Sledování aktuálního stavu a provozu můžeme být schopni detekovat podezřelé činnosti. Na základě toho pak dokážeme zjistit bezpečnostní riziko.
				
				I zde lze využit automatizaci. Například ve zdroji \cite{CoronaGiacintoDetWebAtt} je uveden systém na rozpoznání vzorů zpráv. Koncept založený na znalostním inženýrství může pomoci detekovat i využití zranitelnosti Zero-day\todo{Odkaz na kapitolu} \cite{AhnKimChungBigDataAnal}.
				
				Podobný princip využívají i systémy SIEM (security information and event management). SIEM se zabývá bezpečnostním management. Jeho myšlenka je taková, že  
				rozsáhlé aplikace mají své zdroje informací (logy) na různých místem, ale je zapotřebí k nim přistupovat z jednoho místa. Z tohoto místa se provádí následná analýza kompletně všech zdrojů a vyhodnocují se bezpečnostní rizika \cite{SIEMDef}. Pomocí tohoto vyhodnocení pak může specialozvaný personál reagovat na vzniklou situaci.
				
				\begin{figure}[htb]\centering
					\tmpframe{\includegraphics[width=\textwidth]{./img/todo}}		
					\caption{Grafické znázornění SIEM.}
					\label{fig:siem}
				\end{figure}
			
		\subsection{Zkušenosti z Unify}
			\todo{Popsat jaké zkušenosti mám z unify}
			\todo{Chyba může být nevalidní request, Snaha o umělý request ...}
			
	\section{Platforma Unify}
		\label{sec:unify}
		\todo{Celá tato kapitola se mi nepovedla, je to spíše náčrt.}
		Cílem práce je prověřit jeji funkčnost na platformě Unify \cite{unify}. 
		
		\subsection{Integrační platforma}
			Unify je produkt, který slouží jako integrační platforma. Integrace slouží k propojení různorodých systémů a služeb. 
			
			Integrace se zpravidla zkládají ze sběrnice \cite{mulesoft}. Sběrnice, která propojuje konzumující aplikaci a poskytující aplikace se nazývá \textit{ESB} (enterprise service bus). ESB je stavěno na architektuře orientované na služby \cite{oracleEsb}.
			
			\begin{figure}[htb]\centering
				\tmpframe{\includegraphics[width=\textwidth]{./img/todo}}		
				\caption{Enterprise Service Bus.}
				\label{fig:esb}
			\end{figure}
			
			Konzumující aplikací rozumíme takovou aplikaci, která posílá dotaz na konkrétní službu. Poskytující aplikace je taková aplikace, která poskytuje rozhraní, jehož výstupem jsou data pro různé konzumující aplikace. V reálné aplikaci je možné poskytovat interface, který například spustí nějaký proces.
			
			ESB je využíváno interně v rámci jedné firmy nebo logické struktury. Pokud je žádané, aby některé služby byly konzumovány aplikací takzvaně odjinud používá se sběrnice B2B (business to business) \cite{liaisonB2B}.
			
			B2B poskytuje rozhraní a zpravidla následně samo je konzumentem ESB.
		
		\subsection{O Unify}
		Unify je integrační platforma, která je orientovaná na služby \cite{unify}.
		\todo{Sklada se z B2B, ESB, ETL, SFE + jednoduché popisy}
		
		Unify je postavena na open source technologiích \cite{unify}. Základním stavebním kamenem je ESB, které umožňuje propojit různorodé systémy, služby.
		
		Pro pro připojení externích služeb a aplikací využívá B2B. Technologicky je B2B velmi podobné ESB, ale zpravidla je vhodné ho více zabezpečit, nebo používat validace na požadavky a vyhnout se tím takovým zprávám, jenž neodpovídají ani předem danému předpisu.
		
		Další z mnoha komponent jsou ETL (extract transform load), pomocí kterého dochází k zisku nějakých dat jejich transformaci a nahraní na jíné místo. Nebo SFE (secure file exchange) jehož význam je přenos souborů, hlavně mezi zónami Z2 a Z4. \todo{nemohu si ted vybavit jak se odborně nazývají}.
		   
		Stavební kámen integrační platformy Unify je Jboss AS 7, který zcela podporuje standard Javy EE 6 \cite{oracleJavaEE6}.
		
		
		\todo{Používá se hlavně Java}
		\todo{Podporuje SOAP, REST, HTTP, DB procedury}
		\todo{Rychlé popsání logování}
		\todo{Stávající řešení ochrany - info v GUI, journaling, logy a SL2}
				
		\begin{figure}[htb]\centering
			\tmpframe{\includegraphics[width=\textwidth]{./img/todo}}		
			\caption{High Level Desing architektura Unify.}
			\label{fig:hld_architecture_unify}
		\end{figure}
	
		\blind[1]
		\todo{Zkratky B2B,ESB}
		
	\section{Text mining}
		V logách integrační platformy je veškerá komunikace v textovém formátu. Kromě zpráv ve formátu xml jsou zde i zprávy ve formátu json, popřípadě výpis chyby, pokud nějaká nastala. Protože v prací řeším analýzu takových logů, tak následující kapitola přiblíží těžení znalostí z textu. 
		
		Text mining má velký význam, neboť 80\% informací uložených v počítačích jsou textové formy \cite{IRWebTechniques}.
		
		Text mining je metoda, která těží informace z textu \cite{WittenTextMining}. Přesto, že text mining spadá do kategorie data miningových metod, rozdíl je v tom, že nepracuje s číselnými a většinou ani nominálními hodnotami. Datamining dokáže detekovat skryté informace ze vstupních dat. Text mining informace nemá většinou ve svých datech nijak skryté. Při zpracování textu jde o automatizaci procesu, tak jak by ho zvládl člověk, počítačem. \cite{WittenTextMining}.
		
		Dle zdroje \cite{textAlg} lze probletiku text miningu rozdělit následovně:
		
		\subsection{Získání informací}
			Systémy získání informací identifikují v kolekci souborů takové, jaké jsou vhodné pro vstupní požadavek. Je například o problematiku vyhledávácích nástrojů. Princip je takový, že hledáme-li konkrétní dotaz, jsou vybrány z kolekce všech souborů jen ty, která se dotazu týkají. To se rozhoduje například slovy použitými v dotazu a jejich synonimy.
		
		\subsection{Zpracování přirozeného jazyka}
			Zpracování přirozeného jazyka je jeden z nejtěžších problémů text miningu. V této disciplíně se řeší převod textu do mluveného slova, rozpoznání řeči a podoně. Princip je naučit stroje rozumět přirozenému jazyku. To pomahá například při anotaci souborů.
			
		\subsection{Data mining}
			Data mining je proces, který hledá skryté vzory v datech. V použití s text miningem je využití pro prezentaci různých výsledků koncovému uživateli.
			
		\subsection{Extrakce informací}
			Extrakce informací je proces, kde jsou vytažena data ze vstupu a vložena do logických struktur. \todo{víCe}\cite{SankarSureshTextMining}
		
		\todo{Možná to nad bude lepší v odrážkách než podkapitolách.}
		
		\subsection{Transformace textových dokumentů}
		 Aby bylo možné jednotlivé textové dokumenty klasifikovat nebo shlukovat je třeba je převést na číslný vektor. V této části kapitoly se budu zabývat možnostmi takového převodu.
	
		\subsubsection{TF-IDF}
			\label{subsub:tf-idf}
			TF-IDF je zkratka aglického názvu \textit{Term Frequency Inverse Document Frequency}. Snažší překlad bude, pokud název rozdělíme na Term Frequency, což je v překladu četnost slova a Inverse document frequency, jenž znamená převrácena četnost dokumentu \cite{RamosTF-IDF}.
			
			Četnost slova vyjadříme následovně: máme slovo $w$ a množinu dokumentů $D$, skládající se z dokumentů $d_1, d_2 \ldots d_3 \in D$. Potom četnost slova $TF(w, d_x)$ vyjadřuje kolikrát se slovo $w$ vyskytlo v dokumentu $d_x$.
			
			Převrácená četnost dokumentu vystihuje jak podstatné slovo $w$ je. Značíme ji jako $$IDF(w,D) = \log{\frac{|D|}{|D_w \subseteq D|}}$$ kde $|D|$ vyjadřuje velikost množiny všech dokumentů a $D_w$ množina všech dokumentů, ve kterých se slovo $w$. $|D_w|$ pak značí velikost takové množiny \cite{RamosTF-IDF}.
			
			Pro slovo $w$ v dokumentu $d$ vypočteme TF-IDF následovně:
			$$TF-IDF(w,d,D) = TF(w,d) * IDF(w,D)$$
			
			Na základě uvedeného vzorce jsme schopni reprezentovat textový dokument pomocí vektoru. Vektor takové dokumentu bude vždy nezáporný a bez úprav bude mít tolik dimenzí, kolik má jednoznačných slov v sobě.
			
		\subsubsection{Snížení dimenzionality}
			Při práci s textovými dokumenty je třeba snížit jejich dimenzionalitu. Jak je uvedeno v sekci \ref{subsub:tf-idf} bez úprav dimenzioality TF-IDF pomůže vytvořit vektor o velikosti rovné počtu unikátních slov. Do takové seznamu slov by ovšem v ten moment mohly zapadat slova stejného významu, jen například jinak skloňována. Dalé bude-li pro jednotlivá slova v dokumentu oddělovač mezera, mohou vzniknout jako dvě unikátní slova například \uv{slovo} a \uv{slovo,}.
			
			Před snížením dimenzionality je vhodné zbavit se veškeré interpunkce. Kromě interpunkce je vhodné i text převést kompletně na velká nebo malá písmena.
						
			Pro snížení dimenzionality se používá odstranění takzvaných stop-slov. Jde o slova taková, která nemají velký význam. Přiklad pro to mohou být předložky a spojky. Pro mnoho jazyků jsou k dispozici slovníky s takovými slovy. Dle konkrétní charakteristiky textového dokumentu může být vhodné nadefinovat si svá stop-slova.
			
			Dalším nástrojem pro redukci počtu dimenzí je stemitizace. Cílem stemitizace je redukce slov tím, že jsou převedena na kořen slova \cite{textPreprop}. Je třeba si uvědomit, že tímto krokem je možné ztratit původní význam slova a je tedy třeba promyslet,  jestli na konkrétních datech stemitizaci využít.
		
		
		
		
	\section{Těžení Asociačních pravidel}
		Jednou z prvních myšlenek jak dosáhnout cíle bylo využití metody těžení asociačních pravidel. 
		
		\subsection{Definice}
		Cílem těžení asociačních pravidel je najít zajímavé korelace či časté vzorce v množině dat uložených v relační databázi nebo jiných uložiští \cite{assoc1}. Jako ukázka praktického využití lze představit využití v obchodních řetězcích. Obsah nákupu, který si zákazník zakoupil, je uložen v rámci jedné transakce. Algoritmus vyhledá zajímavé vzorce a pravidla mezi položkami v transakcích. Výsledkem je možnost predikovat co si zákazník zakoupí. Jako příklad uvedeme zákazníka, který si koupil housku a salát. Jako predikce další položky lze očekávat hamburgerové maso. Tato znalost pak lze využít k lepšímu přemístění zboží v regálech či k osobnějším nabídkám zboží v reklamních tiskovinách.
		
		Těžení asociačních pravidel lze dle zdroje \cite{AsocAgrawal1} popsat následovně. Nechť $I = I_1, I_2,\ldots,I_m$ je množina binárníh atributů, kterou budeme nazývat položka. Nechť $T$ je databáze transakcí. Každá transakce $t$ je reprezentována binárním vektorem, kde platí, že $t[k] = 1$ pokud $t$ zakoupila položku $I_k$. V opačném případě $t[k] = 0$. Nechť $X$  je množina některých položek z $I$. Řikáme, že transakce $t$ splňuje $X$ pokud platí pro všechny položky $I_k$ v $X$ pravidlo $t[k] = 1$.	
			
		Asociačním pravidlem rozumíme implikaci $X \implies I_j$, kde $X$ je množina některých položek v množině $I$ a $I_j \in I$ ale zároveň $I_j \notin X$.
		
		Pravidlo $A \implies B$ platí s podporou $S$ v transakci $T$, kde $S(A \implies B) = P(A \cup B)$. Pravidlo $A \implies B$ má v transakci $T$ spolehlivost $C$, kde $C(A \implies B) = P(B|A)$.
		
		Cílem těžení je zjistit vztah mezi různými položkami tak, že přítomnost některých položek v transakci implikuje přítomnost jiné položky.
		
		\subsection{Myšlenka pro využití}
		Myšlenka využití při analýze logu spočívala v tom, že by každá jednotlivá zpráva byla počítána jako transakce. Jednotlivá slova (po předzpracování) by tvořila položky. Pak by bylo možné predikovat, že výskyt některých termů bude znamenat například to, že dojde k odmítnutí nevalidního požadavku. Tyto získané informace by ale neměli nikterak velkou hodnotu. O odmítnutí požadavku se v platformě dozvíme zpravidla v rámci milisekund v synchronní odpovědi.
		
		Velmi pravděpodobně bezpečnostní rizika budou přibývat. Tato možnost by šla využít pro již existující nebo alespoň několikrát se vyskytující případy.
		
		Z těchto důvodů jsem tuto myšlenky zavrhl, stím, že bude lepší vyzkoušet takové metody, které budou mít šanci rozpoznat špatný požadavek, i když ho uvidí poprvé. Tyto metody jsou rozepsány v následujících sekcích. 		
	
	\section{Shluková analýza}
		Jako jednu z možností vyřešení detekce bezpečnostních rizik na základě dat z logů jsem si vybral shlukovou analýzu (dle anglického názvu také nazýván clustering). Clustering využívá znalosti ze vstupních dat k tomu, aby dokázal vstupní požadavky rozdělit do shluků. Všechny informace jsou do těchto shluků přiřazovány na základě podobnosti. Podobnost lze definovat podle charakteristiky dat a požadovém účelu shlukování. Jednotlivé shluky pak tvoří užitečné a logické skupinky .
		
		Všechny objekty ve shluku si jsou navzájem podobné a zároveň se nepodobají objektům v jiném shluku \cite{IntroductionToDataMining}. Na obrázku \ref{fig:clustering} je ukázka několika příkladů, kdy je na stejná data použita shluková analýza s jiným počtem shluků.
		
		\begin{figure}[htb]\centering
			\tmpframe{\includegraphics[width=\textwidth]{./img/todo}}		
			\caption{Ukázka shlukové analýzy s různym počtem shluků.}
			\label{fig:clustering}
		\end{figure}
		
		\todo{Napsat nějaký rozcestník o tom, že dále uvedu tu a tu metodu clusteringu}
		
		\subsection{K-means}
			K-means a K-medoids jsou metody shlukové analýzy. Tyto metody jsou založené na principu centroidů. Centroidem je vyjádřen pomyslný střed každého shluku. V případě K-means jde o střed shluku nezávisle na tom, je-li tento bod i objektem vstupních dat, nebo ne. K-medoids jako střed shluku určí vždy nejvhodnějšího zástupce ze vstupních dat. Graficky je tento rozdíl zobrazen na obrázku. Z obrázku vyplýv, že shluk v K-means může mít střed mimo data, kdežto K-medoids má možnost středem shluku určit pouze nejvhodnější objekt ze vstupních dat \ref{fig:kMeansVSkMedoids}.
			
			\begin{figure}[htb]\centering
				\tmpframe{\includegraphics[width=\textwidth]{./img/todo}}		
				\caption{Ukázka shluku a jeho centroidu pomocí metody K-means a K-medoids.}
				\label{fig:kMeansVSkMedoids}
			\end{figure}
		
			Postup algoritmu pro K-means je nejdříve vybrat $K$ bodů jako centroidy. Tyto centroidy lze vybrat například náhodně, nebo prvních $N$. Volbou prvních centroidů se například zabývá zdroj \cite{kMeansInit}. Po vybrání středů se všechny data přiřadí k nejvhodnějšímu shluku. Výběr takové shluku často ovlivňuje například vzdálenost bodu k centroidu. Následuje přepočet nových středů. Přiřazení do shluků a výpočet nových centroidů se opakuje, dokud shluky mění. Po té považujeme algoritmus za hotový a data rozdělená do skupin.
			
			\todo{Přidat algoritmus.}
			
			Jedním z kroků algoritmu je přiřadit vstupním objektům správný shluk. K tomuto důvodu je nutné definovat funkci pro měření vzdáleností mezi jednotlivými objekty \cite{IntroductionToDataMining}. Požadavky na takovou funkcni jsou, aby byla jednoduchá, protože je velmi často volaná. Pokud data jsou v eukleidovském prostoru používají se například následující funkce na měření vzdálenosti:
			
				\subsubsection{Eukleidova vzdálenost}
				Eukleidova vzdálenost určuje vzdálenost mezi dvěma body v eukleidově prostoru. Výpočet euiklediovy vzdálenosti mezi body $p = (p_1, p_2, \ldots, p_n)$ a $q = (q_1, q_2, \ldots, q_n)$ je
				$$d_e(p,q) = \sqrt{\sum_{i = 1}^{n}(q_i - p_i)^2}$$
				
				\subsubsection{Manhattanská vzdálenost}
				Dalším příkladem měření vzdáleností bodů $p = (p_1, p_2, \ldots, p_n)$ a $q = (q_1, q_2, \ldots, q_n)$ v eukleidově prostoru je manhattanská vzdálenost. Její výpočet je dle vzorce:
				$$d_m(p,q) = \sum_{i = 1}^{n} |p_i - q_i| $$
				
			\todo{Ostatní vzdálenosti}
			\todo{Zmínit, že to je bez učení}
			\todo{Závislé na trénovacích datech}
			
		\subsection{Důvod využití}
			Informace, které integrační platformou prochází jsou velmi různorodé. Požadavky je možné rozdělovat do různých skupin jako například:
			
			\begin{itemize} 
				\item skupiny dle druhu služby (SOAP, REST, databázová \ldots)
				\item skupiny dle služeb 
				\item skupiny dle toho, zdali je zpráva požadavek nebo odpověď		
			\end{itemize}
		
			Jako jedno z možných rozdělení je možné na požadavky v pořádku, podezřelé požadavky a chybné požadavky. Cílem je najít takovou konfiguraci, která by podobné rozdělení dokázala najít. Tedy rozeznat požadavky, které jsou v pořádku (běžná komunikace na integrační platformě.) od těch, jenž jsou podezřelé (ne příliš častý vzor požadavku, \ldots), či zaručeně chybné (chyba protokolu, validace, \ldots). 
		
		\todo{Vysvětlit někde princip komunikace request -> response, popřípadě asynchronní, jms atd.}
			
		
		
		\todo{Proč použít clustering}
		\todo{Druhy clusteringu}
	
	\section{Detekce anomálie}
		Většina požadavků, která projde skrz integrační platformu jsou v pořádku. Chyba či podezřelá zpráva se vyskytují zpravidla minimálně. Proto jsem jako další metodu zvolil detekci anomálie. Detekce anomálie je proces, při kterém se vyhledávají taková data, která se od ostatních výrazněji liší. Detekování odlišností se hojně využívá právě zajišťování bezpečnosti \cite{AnomDetectPCA}. Příklad anomálie je na obrázku \ref{fig:anomalyInData}.
		
		\begin{figure}[htb]\centering
			\tmpframe{\includegraphics[width=\textwidth]{./img/todo}}		
			\caption{Ukázka anomálie v datech.}
			\label{fig:anomalyInData}
		\end{figure}
	
		Princip detekce anomálií je definovat běžná data / chování. Následně veškerá data / chování, která těmto požadavků neodpovídají označit za anomálie \ref{fig:anomalyInData}. 
		
		\subsection{Metody detekce anomálie}
			Detekce anomálií lze rozdělit na tři základní druhy \cite{KumarPCAANom}: 
			\begin{itemize} 
				\item statistické rozdělení
				\item metody založené na vzdálenosti 
				\item metody založené na hustotě		
			\end{itemize}
		
			Statitistické metody očekávají, že data mají průběh nějakého rozdělení. Tomu ja následně přizpůsoben přístup zjišťování anomálií. Zpravidla proto tyto metody nejsou v praxi využívané \cite{KumarPCAANom}.
			
			U metod založených na vzdálenosti se výpočíta vzdálenost konkrétního data a jeho sousedů. Je-li nalezena vzdálenost větší než předem daný práh, je cílový prvek chápán jako anomálie.
			
			Metody založené na hustotě vypočítávají takzvaný LOF (Local Outlier Factor). LOF je hodnota, která u každého prvku určuje míru, jako moc je anomálií \cite{LOF}.
			
			Základ výpočtu LOF je, výpočet vzdálenosti ke $k$ nejbližším sousedům. Tato vzdálenost je použita pro odhad hustoty. Následné porovnání hustoty elementu s hustotami svých $k$ sousedů rozhoduje o tom, bude-li označen za anomálii.
			
			Definice LOF dle \cite{LOF} je popsána pomocí několika dílčích definicích: 
			
			\subsubsection{K-vzdálenost}
				Nechť pro libovolné celé číslo $k$, kde $k > 0$ je určena \textit{k-vzdálenost} objektu $p$ ( $k-dist(p)$ ) jako vzdálenost $d(p, o)$, mezi objektem $p$ a objektem $o \in D$, kde platí:
			
				\begin{itemize} 
					\item pro nejméně $k$ objektů $o' \in D \backslash {p}$	platí, že $d(p, o') \leq d(p, o)$ a zároveň
					\item pro alespoň $k-1$ objektů $o' \in D \backslash {p}$ platí, že $d(p, o') < d(p, o)$
				\end{itemize}
				Množinu $k$ nejbližších sousedů definujeme jako $N_k(p)$
		
			\subsubsection{Dosažitelná vzdálenost}
				Nechť $k$ je přirozené číslo. Dosažitelná vzdálenost objektu $p$ s ohledem na objekt $o$ je definována následovně: 
				$$ reach-dist_k(p, o) = max (k-distance(o), d(p, o)) $$
				
			\subsubsection{Hustota dosažitelnosti}
				Hustota dosažitelnosti objektu $p$ je definována jako
				$$ lrd_{N_k}(p) = \frac{1}{ \frac{\sum_{o \in N_k(p)} reach-dist_k(p,o}{|N_k(p)|}} $$
				
				Hustota dosažitelnosti lze popsat jako inverze průměru dosažitelné vzdálenosti $k$ nejbližších sousedů $p$.
				
			\subsubsection{Činitel anomálie objektu p (LOF}
				LOF objektu $p$ je definován jako 
				
				$$LOF_{N_k}(p) = \frac{\sum_{o \in N_k(p)} \frac{lrd_{N_k}(o)}{lrd_{N_k}(p)}}{|N_k(p)|} $$
			
			
		
		
		\subsection{Analýza hlavních komponent}
		Analýza hlavních komponent, častěji označována jako PCA z anglického překladu \textit{Principal Component Analysis}. \todo{Zkratka PCA} PCA provede analýzu vstupních dat. Tyto data se skládají z mnoha proměnných, které na sobě mohou být závislé. Účelem je vybrat z těchto proměnných důležitou informaci tu reprezentovat jako nové proměnné zvané hlavní komponenty \cite{PCA_book}. Tyto hlavní komponenty jsou lineární kombinací originálních hodnot.
		
		
		
			
	\todo{Být alespoň na straně 20}
	

\chapter{Návrh řešení}
	\todo{Nic moc tento odstavec}
	V této kapitole se zabývám principy, technologiemi algoritmy, které jsem se rozhodl použít, k tomu abych splnil cíle této práce. Tedy vytvoření aplikace, jenž umožní sledovat bezpečností rizika v reálném čase.
	
	\section{Architektura aplikace}
	\todo{Pozor na duplici s odstavcem v úrovni nad}
	\todo{Lze rozdělit na podsekce}
	Požadavkem na aplikaci je, aby požadavky zpracovávala a predikovala v reálném časem. Proto je princip položen na čtení požadavků proudících přes platformu. Jejich předzpracování a odeslání do cloudového řešení Microsoft Azure (Více v sekci \ref{sec:ms_azure}). Po přijetí odpovědi je výsledek uložen do databáze. Pro vizualizaci dat slouží REST API \cite{rest}, které vypisuje předem definované informace ve formátu pro ideální zobrazení v Google Charts \cite{googleCharts}.
	
	Pro snažší představu o architektuře aplikace poslouží obrázek \ref{fig:hld_architecture}, na kterém je vidět High Level Desing \cite{hld_johnson}.
	
	\begin{figure}[htb]\centering
		\tmpframe{\includegraphics[width=\textwidth]{./img/todo}}		
		\caption{High Level Desing architektura aplikace.}
		\label{fig:hld_architecture}
	\end{figure}

	Základem celé aplikace je neohrozit stávající integrační platformu. Na základě toho jsem se rozhodl, že informace o proběhlé komunikaci získám pomocí čtení logovacích souborů.
	Pomocí čtení přírůstků k jednotlivým auditovým logům získám jednotlivé požadavky a pro Unify to nepředstavuje žádnou zátěž.
	
	Dalším stavebním kamenem je použitý aplikační server Jboss ( více v kapitole \ref{sec:jboss}). Na serveru je celá aplikace. Dochází zde k předzpracování zpráv, jejich odslání do Microsoft Azure (\ref{sec:ms_azure}) a také k ukládání do DB.
	
	Jak jsem již zmiňoval provoz z platformy je po zpracování odesílán do MS Azure, zde jsou definovány jednotlivé algoritmy, jejichž výsledky jsou vracený zpět do aplikačního serveru. Azure jsem se rozhodl používát, protože umožňuje rozložení výkonu na server Microsoftu a protože využití služeb v cloudu se stává stále oblíbenějším. Díky spolupráci s Microsoftem je možné i získat, popřípadě zakoupit, instanci Azure do vlastní sítě.
	
	Získané výsledky jsou zpracovány a uloženy do NoSQL databáze MongoDB (více v kapitole \ref{sec:data_storing}).
	
	Aby výsledky nebyly jen hodnoty uložené v databází, je použité REST API, přes které lze výsledky sdílet. API je navržené tak, aby v případě použití Google Charts nevznikly žádné potíže. Google Charts se u cílového zákazníka pužívají již nyní například na zobrazení stavu objednávek. Proto jejich se jejich použití jeví jako další logický krok. Nicméně, není problém stejné api použít pro svoji libovolnou aplikaci, která hodnoty použije buď pro zobrazování přehledů, nebo jako jeden z dalších vstupů například do různých systémů SIEM.
	
	\section{Microsoft Azure}
		\label{sec:ms_azure}
		Na integrační platformě Unify \cite{unify} je předpokládaný provoz 20 požadavků za vteřinu. Vzhledem k takto silnému provozu bude potřeba i přiměřeně velký výpočetní výkon. 
			
		Spolupráce se společností Miscrosof \cite{microsoft} mi umožnola jako řešení vyzkoušet její cloudové služby Microsfot Azure \cite{msAzure}.
			
		Microsoft Azure je sada integrovaných cloudových služeb. Azure nabízí cloudová řešení pro mnoho činností. Motivací k použití této služby k detekci bezpečnostích rizik je nástroj Microsoft Azure Machine Learning Studio \cite{msAzureStudio}.
			
		Microsoft Azure Machine Learning Studio je plně cloudová služba, která umožňuje vytváření prediktivních modelů pro strojové učení \cite{msAzureStudio}. Výhodou studia je to, že veškerý výkon je rozprostřen vevnitř cloudu. Díky grafickému rozhraní je snadné vytvořit učící model, který je následně převeden do modelu prediktivního.
			
		Aby měl prediktivní model smysl, je třeba mu poskytovat nějaká data, u kterých je predikce využita. K tomu se využívají webové služby.
		Prediktivní model se vystaví na specifické URL adrese. Zde je pak očekáván na vstupu konkrétní formát JSONu a služba vrací předem definovanou odpoveď se správnými parametry.
		
		Výhodou využití cloudu je přenesení výpočetní zatěže mimo společnost. Naopak rizikem je problém s konektivitou, který může vytvořit výpadek služby a nebude tedy možné po tuto dobu predikovat rizika. Jednou z možností, jak řešit takové riziko je nechat přenést instanci MS Azure do své sítě.
		
		\todo{Více rozepsat rozdíl mezi učícím a prediktivním modelem}
		
	\section{JBoss}
		\label{sec:jboss}
		Platforma Unify je postavená na aplikačním server JBoss AS 7 \cite{unify}. Z toho důvodu je třeba aby aplikace byla zcela kompatibilní.
		 
		 \todo{Nají nějaké zdroje kde je popsáno o co vlastně jde}
		Jboss AS je aplikační server pro Javu EE\cite{jboss}. 
			
		\todo{Popsat proč jboss}
		\blind[1]	
					
	\section{Logování Unify}
		\label{sec:logging_unify}
		Platforma Unify loguje veškerý průběh do souborových audit logů. Protože celá integrace je založena na Javě, je využit logovací framework Log4j \cite{log4j}. Knihovna Log4j umožňuje nastavit si pattern logování \cite{log4jPatternLayout}. Na Unify je použit pattern \todo{Přidat pattern Log4j}. 
		
		Každý požadavek, který na platformu příde je zalogován do audit.logu. Zpráva je vždy na jedné řádce a zároveň na jedné řádce je povolena pouze jedna.
		
		
		\todo{Ukázka logu}
		
		\todo{Popsat ukázku logu}
		
		Vzhledem k tomuto principu jsem se rozhodl přistoupit k tomu, že se vybrané logovací soubory budou kontinuálně číst, kde se bude ke každému řádku přistupovat jako k samostatné zprávě, která bude následně zpracována dál.
		
		Díky této volbě nebude nutné nijak zasahovat do integrační platformy Unify a minimalizuje se tím riziko, jakéhokoliv nebezpečí ze strany mojí aplikace.
		
		Unify využívá pro některé služby logování do Oracle databáze. Ale vzhledem k tomu, že jde pouze o několik málo určených služeb, připojení na databázy by tak kromě případných komplikací ani nepřineslo žádaný účinek.
		
		
	\section{Ukládání dat}
		\label{sec:data_storing}
		\todo{Porovnání SQL/NoSQL}
		Rozhodl jsem se pro ukládání dat využít NoSQL databázi. Protože aplikace Unify momentálně ukládá veškerý svůj provoz do souborů (vyjímečně jsou některé konkrétní služby journalovány do DB), bude databáze využita i pro ukládání veškeré komunikace. To zpřístupní do budoucna snažší operování s jednotlivými zprávami. Popřípadě snažší zpětnou analýzu.
		\todo{Schéma DB}
		
		Protože se bude ukládat veškerá komunikace, rozhodl jsem se v databázi mít uvedené následující informace:
		
		\todo{Byt je to na pohled jasne, tak popsat co ktery param. je}
		\begin{itemize} 
			\item ObjectId - automaticky generováno z MongoDB
			\item timestamp- časové razítko uložení záznamu do DB
			\item original-message - nezměněná zpráva
			\item normalized-message - zpráva po normalizaci
			\item platformId - jedinečný identifikátor platformy
			\item assignment - skupina, kterou azure vyhodnotil pro zprávu jako správnou 		
		\end{itemize}
					
	\section{Předzpracování dat}
		\label{sec:preprocessing}
		
		\todo{Dopsat čištění dat}
		
		Pro snažší prácí s informacemi z logů jsem se rozhodl pro normalizaci jednotlivých požadavků. Normalizace je jeden z požadavků při zpracovávání textu \cite{txtNrmlztn}.
		
		V kapitole \ref{sec:logging_unify} jsou vidět informace, které se kromě samotné zprávy logují. V každé zprávě se objevuje takzvaná integrační hlavička. V té jsou základní údaje, jako čas odeslání, jednoznačné identifikátory, zdrojové a cilové systémy. Položka jako je timestamp bude zpravidla pro každý požadavek jiná, stejně na tom budou jednoznačné identifikátory. Z tohoto důvodu jsem se rozhodl zvolit jejich nahrazení.
		
		Při normalizaci dat jsem se podobně jako ve zdroji \cite{Li_2013} rozhodl použít následovně:
		
		\begin{itemize} 
			\item Nahrazení všech čísel - Pomocí speciálního symbolu nahradím všechny výskyty čísel.
			\item Velikost písmem - Všechna písmena jsou z velkých znaků převedena na znaky malé.
			\item Odstranění speciálních znaků - Veškeré znaky jako jsou čárka, tečka \ldots jsou odstraněny
			\item Odstranění xml tagů - Rozhodl jsem pro zpracovávat jen obsah zpráv bez xml tagů.	
		\end{itemize}
	
		Nahrazení čísel se mi jeví jako logický krok. V jednotlivejch požadavcích jsou čísla například výsledky různých měření na sítí nebo právě čas v časovém razítku. Pro další využití považuji za podstatné vědět, že se v daném místě vystkytovalo číslo, než že to bylo nějaké konkrétní číslo.
		
		Převod písmen na malá zajistí, aby slova, lišící se právě jen ve velikosti nějakých písmen byla vyhodnocena jako stejná.
		
		Všechna komunikace na platformě je převedena do xml (není-li již od počátku vedena v xml). Protože téměř u všech zpráv stejného druhu se používají ty samé xml tagy, nebudou pro další zpracování podstatné a budou zcela odstraněny. Algoritmus bude dále pracovat jen s reálným obsahem zprávy. 
		
		Na obrázku \ref{fig:normalized_message} je ukázka originálu zprávy a její normalizované alternativy.
		
		\begin{figure}[htb]\centering
			\tmpframe{\includegraphics[width=\textwidth]{./img/todo}}		
			\caption{Original and normalized message}
			\label{fig:normalized_message}
		\end{figure}
			
	\section{Vytvoření vektoru}
		\label{sec:construct_vector}
		V okamžik, kdy máme předzpracovaná, znormalizovaná textová data je nutné najít vhodný způsob pro jejich převod do numerické podoby. To umožní snažší zpracování jak v případě clusteringu, tak i v případě detekce outlinerů.
		
		Cílem tedy je vytvořit vekotr, který bude dostatečně jednotlivé zprávy reprezentovat.
		
		\subsection{TF-IDF algoritmus}
		\label{subsec:tf-idf}
		Při clusteringu dokumentů lze využívat algoritmus TF-IDF (term frequency - inverse document frequency) \cite{Neto_0}. 
		
		\todo{Trošku lépe pořešit}
		\subsubsection{Frekvence slova}
		Ten funguje na principu, že se spočíta frekvence daného slova \textit{w} v dokumentu \textit{d}, označujeme $TF(w,d)$. Vypočteme se tak, že se spočítá suma výskytů slova \textit{w} v dokumentu \textit{d}. Výšší číslo znamená častější výskyt a tedy o to více \textit{w} charakterizuje \textit{d}.
		
		\subsubsection{Frekvence dokumentu}
		Frekvence dokumentu pro slovo \textit{w} DF(w) je počet dokumentů, ve kterých se slovo \textit{w} nachází.
		
		\subsubsection{Inverzní frekvence dokumentu}
		IDF neboli inverzní frekvence dokumentu je daná následující formulí \cite{Ramos_0}:
		
		$$ IDF(w) = \log{\frac{|D|}{DF(w)}}$$  
		
		Kde $|D|$ je počet souborů.
		
		\subsubsection{TF-IDF}
		Samotný vzorec na výpočet TF-IDF je \cite{Neto_0}: 
		
		$$TFIDF(w,d) = TF(w,d) * IDF(w) $$
		
		\subsubsection{Použití}
		Pro své účeli budu předpokládat, že jednotlivé zprávy jsou soubory a slova budou mezerou oddělený obsah zprávy. 
		
		Protože slov může být velké množství, rozhodl jsem se najít nějakou hranici, například takovou, že do výsledného vektoru zanesu TF-IDF pro taková slova, která se vyskytují nejvíce v 95\% zpráv, ale minimálně v 10\%.
		
		\subsection{Forma vektoru}
		V sekci \ref{subsec:tf-idf} jsem navrhl, jak textová data převést do vektoru. Tím je zaručené, že bodou-li se data přenášet přes internet do Microsoft Azure, budou anonymizována. Z vektoru nedokážeme zpětně zprávu vyčíst.
		
		I když z Azure dostáváme synchronně odpověď zpět, a je tedy jasné, ke které zpráve dostávám výsledek, rozhodl jsem se odesílat i jednoznačný identifikátor platformy. To vede k tomu, že pro znalého člověka lze jednotlivé požadavky sledovat i uvnitř MS Azure. Identifikátor sám o sobě vypovídající hodnotu žádnou nemá, ale máme-li k dispozici původní zprávu, jsem ji schopni dohledat.
		
		\todo{Ukázka vektoru}
		
	\section{Konstrukce clusteringu}
		\label{sec:construc_clustering}
		Microsoft Azure nabízí k přípravě experimentů svoje studio dostupné na adrese \url{https://studio.azureml.net}.
		
		Ve studiu Azureml je možné vytvářet své projekty, do projektů umístit své experimenty a ty následně vystavit jako webovou službu.
		
		Základem úspěšného experimentu je vytvořit učící model. To je takový model, pro který máme zvolený cílový algoritmus a na předpřipravených datech ho naučíme aby dokázal v našem případě co nejlépe rozdělovat zprávy do clusterů.
		
		\subsubsection{Předzpracování}
		Veškeré předzpracování a čištění dat probíhá v mojí aplikaci i přesto jsem základní předzpracování zvolil i do experimentu samotného.
		
		Po načtení vstupních dat dochází odstranění duplicitních řádků. Jako další metoda je využití modulu, který smaže řádky, jimž chybí nějaká data.
		
		\todo{Mohl bych použít zároven s klasifikacnim modelem, ale nevim.}
		\subsubsection{Zpracování}
		Pro zpracování jsem zvolil K-Means modul, který je připojený na modul pro trénování clusterovacích modulů.
		
		Po natrénování přiřadíme zbytku testovacích dat clustery a může zhlédnout výsledek.
		
		Na obrázku \ref{fig:k-means_azure} je vidět celý vytvořený trénovací experiment.
		
		
		\begin{figure}[htb]\centering
			\tmpframe{\includegraphics[width=\textwidth]{./img/todo}}	
			\caption{Clustering k-means v prostředí MS AZURE ML Studio.}
			\label{fig:k-means_azure}
		\end{figure}
		
		\todo{Doplnit sem ukázku přiřazení dat a grafy z azure}
		\todo {jak jsem zjistil nejlepší vhodné nastavení}
		
	\section{Konstrukce detekce anomálie}
		\label{sec:construc_anomaly}
		Druhou možností, kterou bych rád vyzkoušel je detekce anomálie.
		To, že chybné požadavky nebo bezpečnostní požadavky se budou výrazněji lišit od běžných zpráv se dá předpokládat.
		
		Princip předzpracování dat v Azureml studiu je stejný jako v při konstrukci modelu pro clustering. Řádky s chybějícími hodnotami a duplikované pro trénování nebudeme používat.
		
		Kromě výše uvedené předzpracující části i zde je část učící a část vyhodnovací.
		
		Trénovací model je vidět na obrázku \ref{fig:anomaly_detection_azure}.
		
		\begin{figure}[htb]\centering
			\tmpframe{\includegraphics[width=\textwidth]{./img/todo}}	
			\caption{Detekce anomálií v prostředí MS AZURE ML Studio.}
			\label{fig:anomaly_detection_azure}
		\end{figure}
		\todo {jak jsem zjistil nejlepší vhodné nastavení}
		
	\section{Prezentace dat}
		\label{sec:data_prezentation}
		Vzhledem k tomu, že by aplikace měla být schopna určovat bezpečnostní rizika, je třeba nějakým způsobem prezentovat její výstupy. Moninitoring na aplikaci Unify je momentálně postaven na tom, že konkrétní lidé hlídají logy a v případě vyskytu chyb, varování nebo jiné netypické události zjišťují co bylo příčinou.
		
		Rozhodl jsem se tedy, že nejlepší bude grafické znázornění. Kromě údajů o tom, že byl zaznamenán požadavek, který je podezřelý budu grafy využívat i k prezentaci základního monitoringu. 
		
		Vzhledem k tomu, že se bude veškerá komunikace ukladát bude vhodné prezentovat například i kolik požadavků na jednotlivé komponentě proběhlo za poslední hodinu a podobně.
		
		Společnost Cetin a.s. \cite{cetin} ve které bude aplikace testována a jenž je uživatelem integrační platformy používá pro různá grafická znázornění grafy od Google Charts\cite{googleCharts}. 
		
		Tyto grafy jsou napsané v jazyce Javascript. Je tedy možné jejich umístění například na intranetové stránky, kde se vysoce postavení lidé společnosti vyznají lépe než v jednotlivých monitorovacích aplikacích.
		
		Na tomto základě jsem se rozhodl vytvořit REST API \cite{rest}, jenž budou Google Charts schopny snadno konzumovat a v případné jiné aplikace, které by stály o podobná data budou schopny se jim přizpůsobit.
		
	\section{Využití dát systémy 3. stran}
		Do budoucna je potřeba počítat s rozšířením monitoringu a je proto vhodné aplikaci připravit tak, aby její výsledky mohly být využity v aplikacích 3. stran.
		
		Lze předpokládat, že k monirování bezpečnosti provozu budou použity systémy SIEM (Security Information and Event Management) \cite{siem}.
		SIEM funguje na principu, kdy zpracovává co nejvíce údajú, na jejichž základě pak rozeznává neočekávané situace a rizika \cite{howDesSiemWork}.
		
		Tím, že jsem se rozhodl data ukládat tak, jak uvádím v kapitole \ref{sec:data_storing} bude libovolný SIEM po připojení do DB schopen získat jak originální zprávu, tak její normalizovanou verzi popřípadě i výsledek vyhodnocení mé aplikace.
		
		Dále je možnost napojit SIEM i na REST API obdobně jako Google Charts v kapitole \ref{sec:data_prezentation}.
		
\chapter{Realizace}
	\todo{Sem vepsat nějaký úvod k této kapitole}
	
	\section{Nutné přípravy pro jboss}
		\subsection{Připravení modulů}
		V aplikaci využívám různé java knihovny, abych k nim měl přístup i v aplikačním serveru, je nutné do něj přidat speciální modul.
		
		Jboss umožňuje snadné přidání modulů. Veškeré moduly jsou umístěny v \textit{wildfly/jboss-eap-7.0/modules/system/layers}. Zde jsem vytvořil svůj modul s konkrétními java knihovnami:
		\begin{itemize} 
			\item commons-codec-1.10.jar
			\item json-simple-1.1.1.jar
			\item mongo-java-driver-3.4.2.jar		
		\end{itemize}
		
		\subsection{Port offset}
		\todo{Je možné, že offset ve finále ještě změním.}
		Dále bylo nutné pro jboss nastavit portový offset. Protože na serveru není jedinou aplikací, je běžný problém v kolizi portů. Z tohoto důvodu jsem zvolil offset 10000. Webové služby tedy místo portu 8080 běží na portu 18080.
		
		\subsection{Zapnutí CORS}
		CORS (Cross-origin resource sharing) neboli \textit{sdílené zdroje odjinud} umožňuje odesílání odpovědí na požadavky z jiné domény \cite{CORS}. V aplikaci je to potřebné pro rest api, kterého se následně dotazuje Google charts.
		
		Corse se v Jboss povoluje v konfiguračním souboru pro standalone aplikaci \textit{standalone.xml} pro doménovou \textit{domain.xml}.
		
	
	\section{Vytvoření modelu na Azure}
		\todo{Ukázka URL}
		\todo{Ukázka Vstupního JSONu a výstupního}
		\todo{V této části bude Prediktivní algoritmus}
		\subsection{Clustering v Azure}
			\todo{Popsat prediktivní experiment}
			\blind[1]
		\begin{figure}[htb]\centering
			\tmpframe{\includegraphics[width=\textwidth]{./img/todo}}	
			\caption{Prediktivní model clusteringu v Azure.}
			\label{fig:training_clustering_azure}
		\end{figure}
		
		\subsection{Detekce anomálií v Azure}
			\todo{Popsat prediktivní experiment}
			\blind[1]
		\begin{figure}[htb]\centering
			\includegraphics[width=\textwidth]{./img/todo}
			\caption{Prediktivní model v detekci anomálií v Azure.}
			\label{fig:training_clustering_azure}
		\end{figure}
	
		\subsection{Webová služba}
			Po dokončení prediktivního modelu je třeba experiment vystavit tak, abychom ho mohli používat z vlastní sítě.
			
			Azure umožňuje takový model spustit jako webovou službu.
			
			  \begin{figure}[htb]\centering
			  	\includegraphics[width=\textwidth]{./img/todo}
			  	\caption{Vytvoření webové služby pomocí stisku tlačítka.}
			  	\label{fig:azure_create_web_servise}
			  \end{figure}
		  
		  Po vytvoření webové služby získáme takzvaný \uv{API key}. Tento řetězec bude sloužit pro přihlášení se do Azure, při dotazování se na konkrétní službu.
		  
		  Také je možné službu otestovat. Otevře se nám okno s očekávanýma políčkama (obr. \ref{fig:azure_test_web_service}). Po vyplnění políček se zobrazí odpověď z prediktivního modelu. Tímto způsobem můžeme otestovat funkčnost nebo pár vzorků. Jiná použití by byla velmi časově a zdrojově nevýhodná.
		  
		  \begin{figure}[htb]\centering
		  	\includegraphics[width=\textwidth]{./img/todo}
		  	\caption{Zobrazené okno pro otestování prediktivního modelu jako webové služby}
		  	\label{fig:azure_test_web_service}
		  \end{figure}
	
	\section{MongoDB}
	\todo{Celkově z této sekce jsem rozpačitý}
		Pro potřeby aplikace je nutné v co nejrychlejším možném čase ukládat jednotlivé requesty. Při ohromném provoze, který se na integrační platformě vyskytuje to je nutná podmínka pro to, aby bylo možné v reálném čase jednotlivé požadavky zpracovávat.
		
		\begin{figure}[htb]\centering
			\tmpframe{\includegraphics[width=0.2\linewidth]{./img/todoSmall}}	
			\caption{Logo MongoDB. \cite{MongoDB}}
			\label{fig:hld_architecture}
		\end{figure}
		
		Rozhodl jsem se za tímto účelem využít MongoDB \cite{MongoDB}, protože se očekává, že bude třeba ukládat v mimořádném případě až 30 záznamů za sekundu, zpravidla bude docházet k více zapisům než čtením.
		
		\subsection{NoSQL}
			MongoDB patří do takzvaných NoSQL databází \cite{mongoDB}. NoSQL v angličtině znamená \uv{Not Only SQL} \cite{Moniruzzaman_2013}, v překladu \uv{Nikoliv pouze SQL}. Jde o skupinu nerelačních databází. Takové databáze nejsou primárně postavené na principu tabulek a zpravidla nepoužívají SQL pro práci s daty \cite{Moniruzzaman_2013}.
			
		\subsection{O MongoDB}
			MongoDB je licencovaná pod  GNU AGPL v3.0 \cite{mongo_gnu} licencí. Data jsou ukládána ve formátu BSON. BSON je binárně zakódovaná JSON \cite{bsonspec.org}.
			
			V MongoDB se vytvářejí kolekce, každá kolekce obsahuje soubory. Soubory mají parametry \cite{mongoDB}. Soubory a jejich parametry lze v čase libovlně měnit nebo přidávat. Což je výhoda, pokud zjistíme, že aktuální návrh není finální, vyhneme se problémům s migrací do nového schématu.
			
			V rámci souboru je možné definovat čítač, který se využije k tomu, aby automaticky generoval jednoznačný identifikátor k souborům nebo lze využít parametr souboru \textbf{\_id}. Ten vygeneruje jednoznačnou identifikaci, ze které jsme schopni například získat i čas vložení dokumentu do kolekce. 
			
		\subsection{Využití v práci}
			\subsubsection{Kolekce terms}
			V práci využívám databázi k ukládání všech slov, ze kterých se tvoří vektor, jenž reprezentuje konkrétní požadavek ( více v kapitole \ref{sec:construct_vector}). Tím není potřeba je mít v paměti a při připadném výpadku je znova vypočítávat.
			
			 V kolekci \textit{terms} ukládám soubory jejichž struktura je automatický identifikátor, slovo pro konstrukci vekotru a timestamp přidání dokumentu do kolekce.
			
			\subsubsection{Kolekce messages}
			Další kolekcí je kolekce \textit{messages}. V té jsou uložené veškeré požadavky, které byly přečteny z logů integrační platformy. Protože ještě před uložením do kolekce dochází v Azure k vyhodnocení, je zpráva uložené i s informací, která určuje zdali je požadavek vyhodnocen jako bezpečností riziko nebo není. 
			
			Struktura každého souboru je: 
			
			\begin{itemize} 
				\item \textbf{\_id} - automaticky generovaný identifikátor
				\item \textbf{timestamp} - čas uložení souboru
				\item \textbf{original-message} - původní požadavek, tak jak byl převzat z logu integrační platformy
				\item \textbf{normalized-message} - požadavek ve znormalizované podobě
				\item \textbf{platform-id} - jednoznačný identifikátor v rámci integrační platformy
				\item \textbf{assignment} - informace od Azure s výsledkem přiřazení katogire 	
			\end{itemize}
		
		\todo{Lépe vysvětlit assignment}
		\todo{Konfigurační kolekce}
		
		\subsection{Práce s MongoDB v Javě}
		V implementaci jsem vytvořil třídu \textit{MongoClientService} (aby bylo možné třídy využívat i v jiných modulech, musí se taková třída skládat z interfacu a jeho implementace, v textu se budu bavit o celku implementace a interfacu dohromady například jako o třídě MongoClientService). Tato třída umožňuje distribuci konkrétní databáZe napříč celou aplikací. 
		
		V jednotlivých modulech si vyvoláme instanci konkrétní databáze a nad tou jsme schopni pracovat. Ovladače pro MongoDB nám umožňují jak data v kládat, tak je číst.
		
		
		
		
	\section{Čtení dat z logů}
		\todo{Popis}				
		Při návrhu zisku jednotlivých požadavků z integrační platformy jsem vycházel z toho, že nový aplikace musí minimálně, či spíše vůbec nezatěžovat Unify \cite{unify}. Vzhledem k tomu, že přes integraci proudí veškerý provoz, je sama o sobě dosti vytížená a v případě, že by touto aplikací byl způsoben výpadek došlo by k silnému ztížení veškerých bussiness procesů, což si nelze dovolit.
		
		Unify veškeré požadavky ukládá do logovacích souborů. Některé, převážně rizikové, služby se zároveň ukládají Oracle databáze. Ale vzhledem k tomu, že nejde o všechny dostupné služby rozhodl jsem se toho nevyužít.
		
		Princip získání dat proudicích přes integrační platformu je založen na čtení jednotlivých logovacích souborů. Jako vhodný nástroj jsem vybral Java třídu Tailer z dostupné knihovny org.apache.commons.io \cite{tailerClass}.
		
		Třída \textit{Tailer}, po implementaci listeneru, se chová stejně jako linuxový příkaz tail \cite{tailLinux}. Průběžně kontroluje čtený soubor a každou nově zapsanou řádku zpracovává.
		
		Tímto řešením získáváme data z integrační platformy, aniž bychom jí zatěžovali. 
		
	\section{Předzpracování a odeslání do Azure}
		\label{sec:send_to_azure}
		\todo{Možná rozdělit na dvě sekce}
		
		Protože jsou data odesílána do cloudu, předzpracováváme je lokálně a přímo do Microsfot Azure odesíláme už jen identifikátor zprávy a vypočtený vektor.
		
		Po přečtení zprávy z auditového logu Unify je zpráva předzpracována (\ref{sec:preprocessing}) a následně je z ní vytvořen vektor (\ref{sec:construct_vector}).
		
		\subsection{Start aplikace}
			První start aplikace je komplikovanější v tom, že pro výpočet finálního vektoru ještě nemáme známá vhodná slova, pro která se budou TF a IDF vypočítávat.
			
			Pro případ, kdy je databáze zcela prázdná jsou nejdříve načteny nějaké zprávy (dle konfigurace), z těch jsou vypočetent vhodné termy.
			V případě, že databáze nějaké zprávy již obsahuje je možné využít je. Nedoporučovaný způsob je vložení slov přímo do databáze. Tato metoda může být vhodná v případě, že například chceme databázi migrovat a nechceme se zdržovat znovu výpočtem.
		
		\subsection{Získání dat z logu}
		 Samotný zisk dat z logu je řešen pomocí Java knihovny Tailer \cite{javaTailer}. Jednou z věcí, které bylo potřeba vyřešit byla situace, kdy třída Tailer během čekání na nový přírustek logovacího souboru zcela zablokovala program. Situaci jsem vyřešil tím, že procesu, který čte z logu integrační platformy jsem pomocí \textit{ExecutorService} \cite{javaExecutorService} umožnil běžet na pozadí aplikace. Tím jsem neblokoval řízení programu.
		 
		\subsection{Předzpracování dat}
		Celý proces předzpracování zpráv probíhá v implementované třídě \textit{LogListener}. Po získání dat, jako textového řetězce, jsou uložena do struktury třídy \textit{AuditLogMessage} (obr \ref{fig:auditLogMessage}). 
		
		Následuje proces vytvoření vektoru, který je odesílán do MS Azure. Vektor se vytváří dle pravidel uvedených v sekci \ref{sec:construct_vector} včetně všech procesů předzpracování . Metody pro výpočet TF a IDF jsou implementované ve třídě \textit{WeightsCounterService}.  
		
		Vektor samotný je reprezentován jako seznam \textit{Double} čísel.
		
		\begin{figure}[htb]\centering
			\tmpframe{\includegraphics[width=0.2\linewidth]{./img/todoSmall}}	
			\caption{Struktura třídy AuditLogMessage.}
			\label{fig:auditLogMessage}
		\end{figure}
		
		Pro odeslání dat bylo třeba vytvořit třídu \textit{AzureWebService}. Vytvoření vhodného požadavku na Azure je podmíněno přihlášením se do služby. Proto do hlavičky je přidána \textit{Basic Access authentication} (jednoduché ověření přístup).
		
		Na požadavek ihned dostaneme synchroní odpověď s výsledkem. Výsledek je zpracován a přiložen k datům načteným z logu.
					
		
	\section{Uložení dat}
		\todo{Popsat v jakém stavu jsou data před ukládáním}
		\todo{Popsat jak se data ukládají}					
		\blind[2]
		
	\section{Napojení na google charts}
		\todo{Popsat jak vypadá javascript}
		\blind[1]
		\todo{Popsat obecné api, které google charts čeká}
		\blind[1]
		\todo{Popsat jak je vyřešené rest API}				
		\blind[1]

\chapter{Analýza a vyhodnocení dat}
	\todo{Sem vepsat nějaký úvod k této kapitole}
	\blind[1]
	\todo{Popsat systém, na který to bylo nasazené}
	\blind[1]
	\section{Analýza K-Means}
		\todo{Popsat vstupní parametry}
		\todo{Ukázat výsledky na Prod logu}	
		\todo{Dojít k závěru}						
		\blind[2]
	\section{Analýza Outliner}
		\todo{Popsat vstupní parametry}
		\todo{Ukázat výsledky na Prod logu}	
		\todo{Dojít k závěru}						
		\blind[2]

\chapter{Závěr}

\setsecnumdepth{part}
\chapter{Conclusion}


\bibliographystyle{iso690}
\bibliography{bibDb}

\setsecnumdepth{all}
\appendix

\chapter{Acronyms}
% \printglossaries
\begin{description}
	\item[GUI] Graphical user interface
	\item[XML] Extensible markup language
\end{description}


\chapter{Contents of enclosed CD}

%change appropriately

\begin{figure}
	\dirtree{%
		.1 readme.txt\DTcomment{the file with CD contents description}.
		.1 exe\DTcomment{the directory with executables}.
		.1 src\DTcomment{the directory of source codes}.
		.2 wbdcm\DTcomment{implementation sources}.
		.2 thesis\DTcomment{the directory of \LaTeX{} source codes of the thesis}.
		.1 text\DTcomment{the thesis text directory}.
		.2 thesis.pdf\DTcomment{the thesis text in PDF format}.
		.2 thesis.ps\DTcomment{the thesis text in PS format}.
	}
\end{figure}

\end{document}
